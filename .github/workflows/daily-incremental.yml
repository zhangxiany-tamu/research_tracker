name: Daily Incremental Paper Sync

on:
  schedule:
    # Run daily at 6 AM UTC
    - cron: '0 6 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  incremental-sync:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Download latest database backup
      uses: dawidd6/action-download-artifact@v3
      with:
        name: database-backup-latest
        path: ./backup/
        github_token: ${{ secrets.GITHUB_TOKEN }}
        repo: ${{ github.repository }}
        workflow_conclusion: success
      continue-on-error: true  # In case no backup exists yet
        
    - name: Restore from backup or create fresh database
      run: |
        if [ -f "./backup/backup_research_tracker.db" ]; then
          echo "ðŸ“¦ Restoring from backup..."
          cp ./backup/backup_research_tracker.db ./research_tracker.db
          echo "âœ… Database restored from backup"
        else
          echo "âš ï¸ No backup found, creating fresh database..."
          python -c "
          from app.database import engine
          from app.models import Base
          Base.metadata.create_all(bind=engine)
          print('âœ… Fresh database created')
          "
        fi
        
    - name: Scrape only NEW papers
      run: |
        python -c "
        from app.database import SessionLocal
        from app.models import Paper, Journal
        from app.scrapers import JASAScraper, JRSSBScraper, BiometrikaScraper, AOSScraper, JMLRScraper
        from app.data_service import DataService
        import json
        from datetime import datetime, timedelta
        
        print('ðŸ”„ Starting incremental scraping for NEW papers only...')
        
        db = SessionLocal()
        data_service = DataService(db)
        
        # Get existing paper titles to avoid duplicates
        existing_titles = set()
        try:
            existing_papers = db.query(Paper.title).all()
            existing_titles = {title[0] for title in existing_papers}
            print(f'ðŸ“Š Found {len(existing_titles)} existing papers in database')
        except:
            print('ðŸ“Š No existing papers found')
        
        scrapers = {
            'JASA': JASAScraper(),
            'JRSSB': JRSSBScraper(), 
            'Biometrika': BiometrikaScraper(),
            'AOS': AOSScraper(),
            'JMLR': JMLRScraper()
        }
        
        total_new_papers = 0
        results = {}
        all_new_papers = []
        
        try:
            for journal_name, scraper in scrapers.items():
                try:
                    print(f'ðŸ“° Checking {journal_name} for new papers...')
                    papers = scraper.scrape_papers()
                    
                    new_papers = []
                    for paper_data in papers:
                        title = paper_data.get('title', '')
                        if title and title not in existing_titles:
                            new_papers.append(paper_data)
                            
                            # Prepare for cloud sync
                            paper_sync_data = {
                                'title': paper_data.get('title'),
                                'authors': paper_data.get('authors', []),
                                'journal': paper_data.get('journal'),
                                'url': paper_data.get('url'),
                                'doi': paper_data.get('doi'),
                                'abstract': paper_data.get('abstract'),
                                'section': paper_data.get('section'),
                                'publication_date': paper_data.get('publication_date').isoformat() if paper_data.get('publication_date') else None,
                                'scraped_date': paper_data.get('scraped_date').isoformat() if paper_data.get('scraped_date') else None
                            }
                            all_new_papers.append(paper_sync_data)
                    
                    # Save new papers to local database
                    saved_count = 0
                    for paper_data in new_papers:
                        success = data_service.save_paper(paper_data)
                        if success:
                            saved_count += 1
                    
                    total_new_papers += saved_count
                    results[journal_name] = {
                        'total_found': len(papers),
                        'new_found': len(new_papers),
                        'saved': saved_count
                    }
                    
                    print(f'âœ… {journal_name}: {saved_count} NEW papers (out of {len(papers)} total)')
                    
                except Exception as e:
                    print(f'âŒ Error scraping {journal_name}: {e}')
                    results[journal_name] = {'error': str(e)}
            
            # Save sync data and results
            with open('new_papers.json', 'w') as f:
                json.dump(all_new_papers, f, indent=2)
                
            with open('incremental_results.json', 'w') as f:
                json.dump({
                    'timestamp': datetime.utcnow().isoformat(),
                    'total_new_papers': total_new_papers,
                    'results': results
                }, f, indent=2)
            
            print(f'ðŸ“Š INCREMENTAL SYNC COMPLETE: {total_new_papers} new papers found')
            
        finally:
            db.close()
        "
        
    - name: Upload updated backup
      uses: actions/upload-artifact@v4
      with:
        name: database-backup-latest
        path: |
          research_tracker.db
          incremental_results.json
        retention-days: 90
        
    - name: Upload timestamped backup
      uses: actions/upload-artifact@v4
      with:
        name: database-backup-${{ github.run_id }}
        path: |
          research_tracker.db
          incremental_results.json
        retention-days: 90
        
    - name: Sync new papers to cloud
      env:
        CLOUD_URL: ${{ secrets.CLOUD_URL || 'https://research-tracker-466018.uc.r.appspot.com' }}
      run: |
        python -c "
        import requests
        import json
        import os
        
        cloud_url = os.getenv('CLOUD_URL', 'https://research-tracker-466018.uc.r.appspot.com')
        
        try:
            # Load new papers
            with open('new_papers.json', 'r') as f:
                new_papers = json.load(f)
            
            if not new_papers:
                print('ðŸ“‹ No new papers to sync')
                exit(0)
            
            print(f'ðŸŒ Syncing {len(new_papers)} NEW papers to cloud...')
            
            # Sync only new papers to cloud
            response = requests.post(
                f'{cloud_url}/api/sync-papers',
                json=new_papers,
                headers={'Content-Type': 'application/json'},
                timeout=300
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f'âœ… Cloud sync successful!')
                print(f'   Synced: {result.get(\"synced_papers\", 0)} new papers')
                print(f'   Updated: {result.get(\"updated_papers\", 0)} existing papers')
                
                # Get updated database stats
                stats_response = requests.get(f'{cloud_url}/api/database-stats', timeout=30)
                if stats_response.status_code == 200:
                    stats = stats_response.json()
                    print(f'ðŸ“Š Updated cloud database stats:')
                    print(f'   Total papers: {stats.get(\"total_papers\", 0)}')
                    for journal, count in stats.get('journal_stats', {}).items():
                        print(f'   {journal}: {count} papers')
                        
            else:
                print(f'âŒ Cloud sync failed: {response.status_code}')
                print(f'Response: {response.text[:500]}')
                exit(1)
                
        except Exception as e:
            print(f'âŒ Cloud sync error: {e}')
            exit(1)
        "
        
    - name: Create summary
      run: |
        echo '## Daily Incremental Sync Summary' >> $GITHUB_STEP_SUMMARY
        echo '' >> $GITHUB_STEP_SUMMARY
        
        python -c "
        import json
        
        try:
            with open('incremental_results.json', 'r') as f:
                results = json.load(f)
            
            total_new = results.get('total_new_papers', 0)
            
            print(f'**Timestamp:** {results[\"timestamp\"]}')
            print(f'**Total NEW Papers Found:** {total_new}')
            print()
            print('### New Papers by Journal')
            print('| Journal | Total Found | NEW Papers | Saved | Status |')
            print('|---------|-------------|------------|-------|--------|')
            
            for journal, result in results['results'].items():
                if 'error' in result:
                    print(f'| {journal} | - | - | - | âŒ Error |')
                else:
                    total_found = result.get('total_found', 0)
                    new_found = result.get('new_found', 0) 
                    saved = result.get('saved', 0)
                    status = 'âœ… Success' if saved > 0 else 'ðŸ“‹ No new papers'
                    print(f'| {journal} | {total_found} | {new_found} | {saved} | {status} |')
            
            print()
            print(f'ðŸŽ¯ **Strategy:** Incremental sync - only adding NEW papers to existing backup')
            print(f'ðŸ—„ï¸ **Updated Backup:** database-backup-latest')
            
        except Exception as e:
            print(f'âŒ Error generating summary: {e}')
        " >> $GITHUB_STEP_SUMMARY